---
title: "Predicting Performance of an Exercise"
author: "ktrebach"
date: "January 24, 2016"
output: 
  html_document:
    keep_md: true
---

#Executive Summary
The goal of this project is to predict the manner in which users perform a particular exercise. This is the "classe" variable in the training set. This report explains the variable selection process, the model selection process, cross validation and an estimate of the expected out of sample error. The final prediction model was used to predict 20 different test cases and I'm pleased to report an accuracy level of 100%.


#Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. Data was collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

#Loading, Reading and Exploring the Data
```{r loaddata, echo=FALSE, message=FALSE, cache=TRUE, results="hide"}

#Download data if it doesn't already exist in directory.

urltrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urltest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
setwd("~/Desktop/Data Scientist JH MOOC/Practical Machine/gitrepos4/practicalmachinelearning/gh-pages")

missing.types <- c("NA", "") # assign NA to all NA and all "".


if(!file.exists("pmltraining.csv")) {
      
      download.file(urltrain,destfile="./pmltraining.csv",method="curl")
      
       } else {
            
            }
training <- read.csv("./pmltraining.csv",stringsAsFactors=TRUE, na.strings=missing.types)

if(!file.exists("pmltraining.csv")) {
      
      download.file(urltest,destfile="./pmltest.csv",method="curl")
      
       } else {
            
            }

test <- read.csv("./pmltest.csv",stringsAsFactors=TRUE, na.strings=missing.types)

## Create a function to format as percent
printpct<-function(x){
      y<-round(x*100,2)
      paste(y,"%",sep="")
}

```


When reading the data, we assign all missing data an NA and we are therefore able to explore the significance (or lack thereof) of when there is NA or missing data.  
 
 
```{r explorationNA, echo=FALSE}
 
#searching for NA data and eliminating variables that have >97% NA/missing

Obstrain <- dim(training)[1]
Obstest <- dim(test)[1]

isNAtrain <- is.na(training)
isNAPct <- colSums(isNAtrain)/Obstrain
maxNAPct <- max(isNAPct)

sumNA <- sum(isNAPct==maxNAPct)
NS<-sum(isNAPct>.97)
notNAnumb <- sum(isNAPct==0)
isNAPctHIGH <-isNAPct[isNAPct>.97]
HIGHNAVAR <- names(isNAPctHIGH)
drops <- c(HIGHNAVAR)

training <- training[,!(names(training) %in% drops)]

```

Each data set has 160 variables.  The training set has `r Obstrain` observations and the testing set has `r Obstest` observations.  There are `r sumNA` variables that have exactly `r printpct(maxNAPct)` of their observations as NA or missing.  It is noted that we only have NAs when new_window is "no".  According to the data documentation, for feature extraction, the authors used a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. We will exclude these variables as they will have little predictive value.


``` {r nearzero search, echo=FALSE}
#searching for and eliminating variables used only for time stamp and record numbers.

suppressMessages(library(caret))
suppressMessages(library(randomForest))

nsv<-nearZeroVar(training,saveMetrics=TRUE) # identify variables that have little to no variability
nsvSum <- sum(nsv$nzv==TRUE) #there is 1. It is new_window.  Also drop time stamp and record number related variables and num_window
drops2 <- c("X","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","new_window","num_window")
training <- training[,!(names(training) %in% drops2)]

```

Further exploration using the nearZeroVar function in the caret package reveals an additional variable, num_window with little to no variability.  The time stamp variables as well do not provide any predictive value and are removed as well.


```{r can we eliminate user name, echo=FALSE, results='hide'}
#Test the hypothesis whether the class is independent of username at .05 significance level.
#H0: Variable A and Variable B are independent. 
#Ha: Variable A and Variable B are not independent.

#If the sample findings are unlikely, given the null hypothesis, the researcher rejects the null hypothesis. Typically, this involves comparing the P-value to the significance level, and rejecting the null hypothesis when the P-value is less than the significance level.


D <- with(training,table(user_name,classe))
DCST <- chisq.test(D)

#As the p-value is extremely tiny (smaller than the .05 significance level), we reject the null hypothesis that the username is independent of the classe.  therefore we must keep it in.

```

We then perform a chi-squared test to see if user_name has any predictive value on classe.  Given the null hypothesis that they are independent of each other, we see from the test that the P-value is extremely tiny and we reject the null hypothesis they are independent and must retain the variable:

```{r printchisq, echo=FALSE}
DCST
```

We will now look for very highly correlated variables.

``` {r explore high correlation variables, echo=FALSE}

M <- abs(cor(training[2:53]))
diag(M) <- 0 #change the 100% correlation to 0 so you can exclude variables correlated with themselves
highcorvar <- which(M > 0.9,arr.ind=TRUE) # which pairs have high correlation


```

We observe that `r dim(highcorvar)[1]` pairs have very high correlation (above 90%), but since it is  difficult to pick which ones to eliminate, we decide to build a predictive model using the variables we have left. If the accuracy is low, we will explore other methods.

We are left with `r dim(training)[2]-1` variables with which to build our predictive model.

#Predictive Model and Testing

Per wikipedia, Random forests ... are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Given the relatively large number of variables that we are using to build our model and that we are aiming to make classifications, I have chosen the random forest method.  

In order to  assess how the results of the random forest method will generalize to an independent data set, a cross validation data set is established. 

```{r settingcrossvalidation, echo=FALSE}

set.seed(12345)
inTrain <- createDataPartition(y=training$classe,
                               p=0.7, list=FALSE)  
training2 <- training[inTrain,]
validation <- training[-inTrain,]


```

Here are the dimensions of the datasets: training2:  `r dim(training2)` and validation: `r dim(validation)`.
 
Given the computational expense of such a large dataset (training2), i will further subset the training2 dataset into a smaller training set (25% of the data) to build the Random Forest model.  The results of the final model call and out of sample error estimate (OOB) are shown below:


```{r randomforest, cache = TRUE, echo=FALSE}

# for computational time saving, partition into a smaller subset

set.seed(23456)
inTrainRF <- createDataPartition(y=training2$classe,
                               p=0.25, list=FALSE)  
trainingRF <- training2[inTrainRF,]
testingRF <- training2[-inTrainRF,]

modFit <- train(classe ~ ., data = trainingRF, method="rf")

modFit$finalModel


```

The results of the model on the balance of the training set is strong with an accuracy above 95%: 


``` {r prediction1, echo=FALSE}

predRF<-predict(modFit,testingRF)
confusionMatrix(predRF,testingRF$classe)

```

We now perform the cross validation on the validation test set and the results are also strong as expected with an accuracy rate also above 95%.


```{r crossvalidation, echo=FALSE}

predRFCV<-predict(modFit,validation)
confusionMatrix(predRFCV,validation$classe)


```

#Final Results
The final model was applied to the test set and the predicted classes had an accuracy of 100%.

```{r for quiz, echo=FALSE, results='hide'}
#preprocess test; remove unneeded variables

testF <- test[,!(names(test) %in% drops)]
testF <- testF[,!(names(testF) %in% drops2)]

predQuiz<-predict(modFit,testF)


```

